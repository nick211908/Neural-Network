{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F999hEjlE125"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network: Accuracy Fixes and Best Practices\n",
        "\n",
        "- Ensured correct one-hot encoding shape (no unnecessary transpose)\n",
        "- Added shuffling of training data at each epoch\n",
        "- Set random seed for reproducibility\n",
        "- Improved comments for clarity\n",
        "- Retained correct linear algebra in Layer class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twy_vxQtE9RJ",
        "outputId": "94287e9b-0b4e-46d1-cf5c-1008bdca9544"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv(\"C:\\Users\\Acer\\Desktop\\Neural-Network\\Data\\mnist_train.csv\")\n",
        "data=np.array(data)\n",
        "m,n=data.shape\n",
        "np.random.shuffle(data)\n",
        "print(m,n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmCJ5lHefOKu",
        "outputId": "e1818ab3-1124-488b-f716-ff80060556b9"
      },
      "outputs": [],
      "source": [
        "train_data=data[0:int(0.8*m),:]\n",
        "val_data=data[int(0.8*m):m,:]\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6GJlKMelY22"
      },
      "outputs": [],
      "source": [
        "X_train=train_data[:,1:].T\n",
        "X_train=X_train/255.0\n",
        "Y_train=train_data[:,0].astype('int8')\n",
        "X_val=val_data[:,1:].T\n",
        "X_val=X_val/255.0\n",
        "Y_val=val_data[:,0].astype('int8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6n7HoeEpQRU"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(y, num_classes=10):\n",
        "    one_hot = np.zeros((num_classes, y.size))\n",
        "    one_hot[y, np.arange(y.size)] = 1\n",
        "    return one_hot\n",
        "\n",
        "Y_train_encoded = one_hot_encode(Y_train)\n",
        "Y_val_encoded = one_hot_encode(Y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v01tPJsoosJn"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self, n_input, n_neuron):\n",
        "        # He initialization\n",
        "        self.w = np.random.randn(n_neuron, n_input) * np.sqrt(2.0/n_input)\n",
        "        self.b = np.zeros((n_neuron, 1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.output = np.dot(self.w, inputs) + self.b\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dweights = np.dot(dvalues, self.inputs.T)\n",
        "        self.dbias = np.sum(dvalues, axis=1, keepdims=True)\n",
        "        self.dinputs = np.dot(self.w.T, dvalues)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2gCf8GMnLyV"
      },
      "outputs": [],
      "source": [
        "class Activation_ReLU:\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dinputs = dvalues.copy()\n",
        "        self.dinputs[self.inputs <= 0] = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AirGQFJwmpa"
      },
      "outputs": [],
      "source": [
        "class Activation_Softmax:\n",
        "    def forward(self, inputs):\n",
        "        # Stability trick: subtract max from each column\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=0, keepdims=True))\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
        "        self.output = probabilities\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        # Placeholder for now â€” not used directly (combined with loss later)\n",
        "        self.dinputs = dvalues.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z01DXC9fx6Mu"
      },
      "outputs": [],
      "source": [
        "class Loss_CategoricalCrossEntropy:\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
        "\n",
        "        # Case 1: One-hot encoded labels\n",
        "        if len(y_true.shape) == 2:\n",
        "            # y_pred: (10, N), y_true: (10, N)\n",
        "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=0)\n",
        "\n",
        "        # Case 2: integer labels (not used here, but safe to keep)\n",
        "        else:\n",
        "            correct_confidences = y_pred_clipped[y_true, np.arange(y_pred.shape[1])]\n",
        "\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return np.mean(negative_log_likelihoods)\n",
        "\n",
        "    def backward(self, dvalues, y_true):\n",
        "      samples = dvalues.shape[1]\n",
        "\n",
        "      # If one-hot encoded, convert to class indices\n",
        "      if len(y_true.shape) == 2:\n",
        "          y_true = np.argmax(y_true, axis=0)\n",
        "\n",
        "      # Gradient of softmax + crossentropy combined\n",
        "      self.dinputs = dvalues.copy()\n",
        "      self.dinputs[y_true, np.arange(samples)] -= 1\n",
        "      self.dinputs = self.dinputs / samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0DHq0JP7epx"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, lr=0.1):\n",
        "        self.layer1 = Layer(input_size, hidden_size1)\n",
        "        self.activation1 = Activation_ReLU()\n",
        "\n",
        "        self.layer2 = Layer(hidden_size1, hidden_size2)\n",
        "        self.activation2 = Activation_ReLU()\n",
        "\n",
        "        self.layer3 = Layer(hidden_size2, output_size)\n",
        "        self.activation3 = Activation_Softmax()\n",
        "\n",
        "        self.loss_function = Loss_CategoricalCrossEntropy()\n",
        "        self.learning_rate = lr\n",
        "\n",
        "    def forward(self, X, y):\n",
        "        self.layer1.forward(X)\n",
        "        self.activation1.forward(self.layer1.output)\n",
        "\n",
        "        self.layer2.forward(self.activation1.output)\n",
        "        self.activation2.forward(self.layer2.output)\n",
        "\n",
        "        self.layer3.forward(self.activation2.output)\n",
        "        self.activation3.forward(self.layer3.output)\n",
        "\n",
        "        loss = self.loss_function.forward(self.activation3.output, y)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, y):\n",
        "        self.loss_function.backward(self.activation3.output, y)\n",
        "        self.activation3.backward(self.loss_function.dinputs)\n",
        "\n",
        "        self.layer3.backward(self.activation3.dinputs)\n",
        "        self.activation2.backward(self.layer3.dinputs)\n",
        "\n",
        "        self.layer2.backward(self.activation2.dinputs)\n",
        "        self.activation1.backward(self.layer2.dinputs)\n",
        "\n",
        "        self.layer1.backward(self.activation1.dinputs)\n",
        "\n",
        "    def update(self):\n",
        "        # Gradient Descent\n",
        "        for layer in [self.layer1, self.layer2, self.layer3]:\n",
        "            layer.w -= self.learning_rate * layer.dweights\n",
        "            layer.b -= self.learning_rate * layer.dbias\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.layer1.forward(X)\n",
        "        self.activation1.forward(self.layer1.output)\n",
        "\n",
        "        self.layer2.forward(self.activation1.output)\n",
        "        self.activation2.forward(self.layer2.output)\n",
        "\n",
        "        self.layer3.forward(self.activation2.output)\n",
        "        self.activation3.forward(self.layer3.output)\n",
        "\n",
        "        return np.argmax(self.activation3.output, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wf2ZAgs57nEo",
        "outputId": "dce8242f-9fca-4e0a-ee8f-9f74b0d0d4fa"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Initialize model with larger hidden layers and smaller learning rate\n",
        "model = NeuralNetwork(784, 128, 64, 10, lr=0.01)\n",
        "\n",
        "def get_batch(X, y, batch_size):\n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "        yield X[:, i:i+batch_size], y[:, i:i+batch_size]\n",
        "\n",
        "# Training parameters\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "best_val_acc = 0\n",
        "patience = 5\n",
        "no_improve = 0\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "   \n",
        "    perm = np.random.permutation(X_train.shape[1])\n",
        "    X_train = X_train[:, perm]\n",
        "    Y_train_encoded = Y_train_encoded[:, perm]\n",
        "    Y_train = Y_train[perm]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    batch_count = 0\n",
        "    \n",
        "    # Training\n",
        "    for X_batch, y_batch in get_batch(X_train, Y_train_encoded, batch_size):\n",
        "        loss = model.forward(X_batch, y_batch)\n",
        "        model.backward(y_batch)\n",
        "        model.update()\n",
        "        epoch_loss += loss\n",
        "        batch_count += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / batch_count\n",
        "    \n",
        "    # Calculate training accuracy\n",
        "    train_preds = model.predict(X_train)\n",
        "    train_acc = np.mean(train_preds == Y_train)\n",
        "    \n",
        "    # Calculate validation accuracy\n",
        "    val_preds = model.predict(X_val)\n",
        "    val_acc = np.mean(val_preds == Y_val)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        \n",
        "    if no_improve >= patience:\n",
        "        print(f\"Early stopping triggered. Best validation accuracy: {best_val_acc:.4f}\")\n",
        "        break\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
